{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098587a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar las librerías necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sweetviz as sv\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import Parallel, delayed\n",
    "import warnings\n",
    "from scipy.stats import zscore, mode\n",
    "from sklearn.metrics import (silhouette_samples,silhouette_score,make_scorer,mean_absolute_error, r2_score, mean_squared_error,accuracy_score,precision_score,recall_score,f1_score,roc_auc_score)\n",
    "from sklearn.base import (BaseEstimator,TransformerMixin,ClassifierMixin,RegressorMixin)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, BaseCrossValidator, KFold, cross_val_score, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import joblib\n",
    "from joblib import load\n",
    "import os\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "from tslearn.clustering import TimeSeriesKMeans, silhouette_score as ts_silhouette_score\n",
    "import shap\n",
    "from lime import lime_tabular\n",
    "from sklearn.model_selection import GroupKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6a628b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los datos desde el archivo Excel\n",
    "file_path = 'C:/Users/Andy/OneDrive/Desktop/MCD/Tesis/Datos_fuente_Bloomberg/en valores/serie completa 2014-2024/Dataset/dataset_completo.xlsx'\n",
    "df = pd.read_excel(file_path, sheet_name='dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b6ef82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78204f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcda13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quitar \"k\" de las columnas \"Non farm payrolls_Exp_mediana\" y \"Non farm payrolls_Exp_promedio\"\n",
    "df['Non farm payrolls_Exp_mediana'] = df['Non farm payrolls_Exp_mediana'].astype(str).str.replace('k', '')\n",
    "df['Non farm payrolls_Exp_promedio'] = df['Non farm payrolls_Exp_promedio'].astype(str).str.replace('k', '')\n",
    "\n",
    "# Convertir las columnas a valores numéricos\n",
    "df['Non farm payrolls_Exp_mediana'] = pd.to_numeric(df['Non farm payrolls_Exp_mediana'], errors='coerce')\n",
    "df['Non farm payrolls_Exp_promedio'] = pd.to_numeric(df['Non farm payrolls_Exp_promedio'], errors='coerce')\n",
    "df['P_B'] = pd.to_numeric(df['P_B'], errors='coerce')\n",
    "df['P_TB'] = pd.to_numeric(df['P_TB'], errors='coerce')\n",
    "df['P_S'] = pd.to_numeric(df['P_S'], errors='coerce')\n",
    "df['P_FCF'] = pd.to_numeric(df['P_FCF'], errors='coerce')\n",
    "df['P_Share'] = pd.to_numeric(df['P_Share'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceeed879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir la columna de fecha a formato datetime\n",
    "df['Fecha'] = pd.to_datetime(df['Fecha'], format='%Y%m%d')\n",
    "\n",
    "# Extraer características temporales relevantes\n",
    "df['mes'] = df['Fecha'].dt.month\n",
    "df['año'] = df['Fecha'].dt.year\n",
    "df['periodo'] = df['Fecha'].dt.to_period('M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8bb448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir CPI y Fed Funds Rate a decimales\n",
    "df['CPI'] = df['CPI'] / 100\n",
    "df['Fed Funds Rate'] = df['Fed Funds Rate'] / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b63961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular diferencias en puntos básicos\n",
    "df['dif_CPI_mediana'] = (df['CPI'] - df['CPI_Exp_mediana']) * 10000\n",
    "df['dif_CPI_promedio'] = (df['CPI'] - df['CPI_Exp_promedio']) * 10000\n",
    "\n",
    "df['dif_FFR_mediana'] = (df['Fed Funds Rate'] - df['Fed Funds Rate_Exp_mediana']) * 10000\n",
    "df['dif_FFR_promedio'] = (df['Fed Funds Rate'] - df['Fed Funds Rate_Exp_promedio']) * 10000\n",
    "\n",
    "# Calcular diferencia\n",
    "df['dif_NFP_mediana'] = (df['Non farm payrolls'] - df['Non farm payrolls_Exp_mediana'])\n",
    "df['dif_NFP_promedio'] = (df['Non farm payrolls'] - df['Non farm payrolls_Exp_promedio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235fe622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tickers financieros (GICS)\n",
    "gics_financieras = r'C:\\Users\\Andy\\OneDrive\\Desktop\\MCD\\Tesis\\Datos_fuente_Bloomberg\\GICS_Finanzas.xlsx'\n",
    "try:\n",
    "    df_tickers_financieras = pd.read_excel(gics_financieras)\n",
    "    columna_ticker_excel = 'Ticker de miembro'\n",
    "    columna_ticker_df = 'Ticker de miembro'\n",
    "\n",
    "    if columna_ticker_excel not in df_tickers_financieras.columns:\n",
    "        raise ValueError(f\"La columna '{columna_ticker_excel}' no se encuentra en el archivo Excel.\")\n",
    "\n",
    "    # Seleccionar solo la columna de tickers y renombrarla si es necesario para el merge\n",
    "    df_tickers_financieras = df_tickers_financieras[[columna_ticker_excel]].rename(columns={columna_ticker_excel: columna_ticker_df})\n",
    "    # Eliminar duplicados por si acaso\n",
    "    df_tickers_financieras = df_tickers_financieras.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    print(f\"Se cargaron {len(df_tickers_financieras)} tickers del sector financiero.\")\n",
    "    print(df_tickers_financieras.head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: No se encontró el archivo '{gics_financieras}'\")\n",
    "    # Detener o manejar el error\n",
    "except ValueError as ve:\n",
    "     print(f\"Error al procesar el Excel: {ve}\")\n",
    "     # Detener o manejar el error\n",
    "except Exception as e:\n",
    "    print(f\"Ocurrió un error inesperado al leer el Excel: {e}\")\n",
    "    # Detener o manejar el error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1177aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "columna_ticker_df_en_principal = 'Empresa' \n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "print(f\"\\nFiltrando el DataFrame principal ('df') para tickers financieros...\")\n",
    "print(f\"Se usarán los tickers de la columna '{df_tickers_financieras.columns[0]}' del archivo Excel.\")\n",
    "print(f\"Se buscarán coincidencias en la columna '{columna_ticker_df_en_principal}' del DataFrame principal.\")\n",
    "\n",
    "# Obtener la lista única de tickers financieros cargados\n",
    "tickers_financieros_lista = df_tickers_financieras[df_tickers_financieras.columns[0]].unique()\n",
    "\n",
    "# Verificar que la columna exista en df\n",
    "if columna_ticker_df_en_principal not in df.columns:\n",
    "    print(f\"ERROR: La columna '{columna_ticker_df_en_principal}' no existe en el DataFrame principal 'df'.\")\n",
    "    print(f\"Columnas disponibles en 'df': {df.columns.tolist()}\")\n",
    "    # Detener o manejar el error\n",
    "    # df_financieras_completo = pd.DataFrame() # Crear df vacío para evitar errores posteriores\n",
    "else:\n",
    "    # Filtrar df usando la lista de tickers\n",
    "    # Usamos .isin() para encontrar filas donde el ticker en df está en nuestra lista\n",
    "    df_financieras_completo = df[df[columna_ticker_df_en_principal].isin(tickers_financieros_lista)].copy()\n",
    "\n",
    "    num_empresas_filtradas = df_financieras_completo[columna_ticker_df_en_principal].nunique()\n",
    "    print(f\"\\nDataFrame filtrado ('df_financieras_completo') contiene:\")\n",
    "    print(f\"  - {len(df_financieras_completo)} filas\")\n",
    "    print(f\"  - {num_empresas_filtradas} empresas financieras únicas.\")\n",
    "\n",
    "    if df_financieras_completo.empty:\n",
    "        print(\"\\n¡ADVERTENCIA! El DataFrame filtrado está vacío.\")\n",
    "        print(\"Posibles causas:\")\n",
    "        print(\"  - No hay coincidencias entre los tickers del Excel y los tickers en la columna \"\n",
    "              f\"'{columna_ticker_df_en_principal}' de 'df'.\")\n",
    "        print(\"  - Verifica los formatos de los tickers (ej. 'AAPL' vs 'AAPL US Equity').\")\n",
    "        print(\"  - Verifica que el nombre de la columna en 'df' sea correcto.\")\n",
    "    else:\n",
    "        print(\"\\nPrimeras filas del DataFrame filtrado ('df_financieras_completo'):\")\n",
    "        print(df_financieras_completo.head())\n",
    "        print(\"\\nÚltimas filas del DataFrame filtrado ('df_financieras_completo'):\")\n",
    "        print(df_financieras_completo.tail())\n",
    "\n",
    "# --- LISTO PARA EL SIGUIENTE PASO ---\n",
    "# Ahora puedes usar 'df_financieras_completo' como input para\n",
    "# el bloque que crea 'dfs_shifted_financieras' (con lag 1 y manejo de NaNs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba3531c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de columnas numéricas para análisis con mensualización lineal de los ratios \n",
    "# y variables macroeconómicas calculadas con mediana\n",
    "columnas_numericas_lineal_mediana= [\n",
    "    'P_E',\n",
    "    'P_B',\n",
    "    'P_S',\n",
    "    'P_Share',\n",
    "    'ROCE_l',\n",
    "    'EBIT_l',\n",
    "    'Total Activos_l',\n",
    "    'Deuda a LP_l',\n",
    "    'ROA_l',\n",
    "    'Beneficio neto_l',\n",
    "    'ROI_l',\n",
    "    'EV_l',\n",
    "    'Cap de mercado_l',\n",
    "    'Deuda a CP_l',\n",
    "    'Efectivo y equiv_l',\n",
    "    'dif_CPI_mediana',\n",
    "    'dif_FFR_mediana',\n",
    "    'dif_NFP_mediana'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a94923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de columnas numéricas para análisis con mensualización spline de los ratios \n",
    "# y variables macroeconómicas calculadas con mediana\n",
    "columnas_numericas_spline_mediana = [\n",
    "    'P_E',\n",
    "    'P_B',\n",
    "    'P_S',\n",
    "    'P_Share',\n",
    "    'ROCE_sp',\n",
    "    'EBIT_sp',\n",
    "    'Total Activos_sp',\n",
    "    'Deuda a LP_sp',\n",
    "    'ROA_sp',\n",
    "    'Beneficio neto_sp',\n",
    "    'ROI_sp',\n",
    "    'EV_sp',\n",
    "    'Cap de mercado_sp',\n",
    "    'Deuda a CP_sp',\n",
    "    'Efectivo y equiv_sp',\n",
    "    'dif_CPI_mediana',\n",
    "    'dif_FFR_mediana',\n",
    "    'dif_NFP_mediana'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b651c77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de columnas numéricas para análisis con mensualización lineal de los ratios \n",
    "# y variables macroeconómicas calculadas con promedio\n",
    "columnas_numericas_lineal_promedio = [\n",
    "    'P_E',\n",
    "    'P_B',\n",
    "    'P_S',\n",
    "    'P_Share',\n",
    "    'ROCE_l',\n",
    "    'EBIT_l',\n",
    "    'Total Activos_l',\n",
    "    'Deuda a LP_l',\n",
    "    'ROA_l',\n",
    "    'Beneficio neto_l',\n",
    "    'ROI_l',\n",
    "    'EV_l',\n",
    "    'Cap de mercado_l',\n",
    "    'Deuda a CP_l',\n",
    "    'Efectivo y equiv_l',\n",
    "    'dif_CPI_promedio',\n",
    "    'dif_FFR_promedio',\n",
    "    'dif_NFP_promedio'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82c79c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de columnas numéricas para análisis con mensualización spline de los ratios \n",
    "# y variables macroeconómicas calculadas con promedio\n",
    "columnas_numericas_spline_promedio = [\n",
    "    'P_E',\n",
    "    'P_B',\n",
    "    'P_S',\n",
    "    'P_Share',\n",
    "    'ROCE_sp',\n",
    "    'EBIT_sp',\n",
    "    'Total Activos_sp',\n",
    "    'Deuda a LP_sp',\n",
    "    'ROA_sp',\n",
    "    'Beneficio neto_sp',\n",
    "    'ROI_sp',\n",
    "    'EV_sp',\n",
    "    'Cap de mercado_sp',\n",
    "    'Deuda a CP_sp',\n",
    "    'Efectivo y equiv_sp',\n",
    "    'dif_CPI_promedio',\n",
    "    'dif_FFR_promedio',\n",
    "    'dif_NFP_promedio'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ba2252",
   "metadata": {},
   "outputs": [],
   "source": [
    "listas_columnas = {\n",
    "    'lineal_mediana': columnas_numericas_lineal_mediana,\n",
    "    'spline_mediana': columnas_numericas_spline_mediana,\n",
    "    'lineal_promedio': columnas_numericas_lineal_promedio,\n",
    "    'spline_promedio': columnas_numericas_spline_promedio\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b82176d",
   "metadata": {},
   "outputs": [],
   "source": [
    "listas_columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5293d457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ┌─────────────────────────────────────────────────────────────────┐\n",
    "# │ BLOQUE 1: Importaciones y Funciones Auxiliares                  │\n",
    "# └─────────────────────────────────────────────────────────────────┘\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def pct_missing_by_column(df, columns):\n",
    "    \"\"\"Devuelve % de NaNs por columna, ordenado descendente.\"\"\"\n",
    "    return (df[columns].isna().mean() * 100).sort_values(ascending=False)\n",
    "\n",
    "def pct_missing_by_group(df, group_col, features):\n",
    "    \"\"\"Devuelve Series con el máximo % de NaNs en cualquier feature por grupo.\"\"\"\n",
    "    missing = df.groupby(group_col)[features]\\\n",
    "                .apply(lambda g: g.isna().mean()*100)\n",
    "    return missing.max(axis=1)\n",
    "\n",
    "def filter_companies_by_target_missing(df, group_col, target_col):\n",
    "    \"\"\"Filtra y devuelve df sin empresas que tengan ANY NaN en target_col.\"\"\"\n",
    "    ok = ~df.groupby(group_col)[target_col].apply(lambda s: s.isna().any())\n",
    "    valid_companies = ok[ok].index\n",
    "    return df[df[group_col].isin(valid_companies)].copy()\n",
    "\n",
    "def impute_by_group(df, group_col, features, methods=('ffill',), fill_value=0): # <-- Cambiar default\n",
    "    \"\"\"Imputa NaNs por grupo usando .ffill() y termina con fillna(fill_value).\"\"\"\n",
    "    df2 = df.sort_values([group_col, 'Fecha']).copy()\n",
    "    for m in methods:\n",
    "        df2[features] = df2.groupby(group_col)[features]\\\n",
    "                            .transform(lambda g: getattr(g, m)())\n",
    "    return df2.fillna(fill_value)\n",
    "\n",
    "def create_lags(df, group_col, date_col, features, lag=1):\n",
    "    \"\"\"Genera columnas de lag para cada feature dentro de cada grupo.\"\"\"\n",
    "    df2 = df.sort_values([group_col, date_col]).copy()\n",
    "    for feat in features:\n",
    "        df2[f\"{feat}_lag{lag}\"] = df2.groupby(group_col)[feat].shift(lag)\n",
    "    return df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ef57a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ┌─────────────────────────────────────────────────────────────────┐\n",
    "# │ BLOQUE 2: Filtrar Empresas Financieras                         │\n",
    "# └─────────────────────────────────────────────────────────────────┘\n",
    "# Asume: df y df_tickers_financieras ya cargados, con la columna ticker en df_tickers_financieras.columns[0]\n",
    "ticker_col = df_tickers_financieras.columns[0]\n",
    "df_fin = df[df['Empresa'].isin(df_tickers_financieras[ticker_col].unique())].copy()\n",
    "print(f\"Empresas financieras: {df_fin['Empresa'].nunique()} únicas, {len(df_fin)} filas\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d584f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ┌─────────────────────────────────────────────────────────────────┐\n",
    "# │ BLOQUE 3: Conversión Masiva a Numérico                         │\n",
    "# └─────────────────────────────────────────────────────────────────┘\n",
    "# Crear set de features base (sin 'Empresa' ni 'Fecha'), añadir 'P_E' si existe\n",
    "features_base = {\n",
    "    col for cols in listas_columnas.values()\n",
    "            for col in cols if col not in ('Empresa','Fecha')\n",
    "}\n",
    "if 'P_E' in df_fin.columns:\n",
    "    features_base.add('P_E')\n",
    "numeric_cols = [c for c in sorted(features_base) if c in df_fin.columns]\n",
    "\n",
    "# Vectorizado\n",
    "nan_before = df_fin[numeric_cols].isna().sum()\n",
    "df_fin[numeric_cols] = df_fin[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
    "nan_after = df_fin[numeric_cols].isna().sum()\n",
    "print(\"NaNs añadidos:\", (nan_after - nan_before).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36c3b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ┌─────────────────────────────────────────────────────────────────┐\n",
    "# │ BLOQUE 4: Excluir Columnas Críticas con Demasiados NaNs        │\n",
    "# └─────────────────────────────────────────────────────────────────┘\n",
    "col_pct = pct_missing_by_column(df_fin, numeric_cols)\n",
    "col_pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9d9e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "umbral_col = 20.0   # % máximo de NaNs tolerable\n",
    "cols_to_exclude = col_pct[col_pct > umbral_col].index.tolist()\n",
    "features_kept = [c for c in numeric_cols if c not in cols_to_exclude]\n",
    "print(f\"Columnas excluidas (> {umbral_col}% NaNs):\", cols_to_exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7052f10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ┌─────────────────────────────────────────────────────────────────┐\n",
    "# │ BLOQUE 5: Filtrar Empresas con NaNs en el Target P_E           │\n",
    "# └─────────────────────────────────────────────────────────────────┘\n",
    "df_fin_no_PE_NaN = filter_companies_by_target_missing(df_fin, 'Empresa', 'P_E')\n",
    "print(\"Empresas tras filtrar P_E NaN:\", df_fin_no_PE_NaN['Empresa'].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55da67e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ┌─────────────────────────────────────────────────────────────────┐\n",
    "# │ BLOQUE 6: Excluir Empresas con Demasiados NaNs en Features     │\n",
    "# └─────────────────────────────────────────────────────────────────┘\n",
    "umbral_grp = 10.0   # % máximo de NaNs por empresa\n",
    "grp_pct = pct_missing_by_group(df_fin_no_PE_NaN, 'Empresa', features_kept)\n",
    "empresas_final = grp_pct[grp_pct <= umbral_grp].index.tolist()\n",
    "df_fin_final = df_fin_no_PE_NaN[df_fin_no_PE_NaN['Empresa'].isin(empresas_final)].copy()\n",
    "print(\"Empresas finales (<= {umbral_grp}% NaNs):\", len(empresas_final))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88f3e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ┌─────────────────────────────────────────────────────────────────┐\n",
    "# │ BLOQUE 7: Imputar NaNs Restantes                                │\n",
    "# └─────────────────────────────────────────────────────────────────┘\n",
    "df_imputed = impute_by_group(df_fin_final, 'Empresa', features_kept)\n",
    "print(\"NaNs totales después de imputar:\", df_imputed[features_kept].isna().sum().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6b57aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ┌─────────────────────────────────────────────────────────────────┐\n",
    "# │ BLOQUE 8: Crear DataFrames con Lag (corregido)                │\n",
    "# └─────────────────────────────────────────────────────────────────┘\n",
    "dfs_shifted = {}\n",
    "for metodo, cols in listas_columnas.items():\n",
    "    # Excluir P_E de la lista de feats para no duplicarlo\n",
    "    feats = [c for c in cols \n",
    "             if c in df_imputed.columns \n",
    "             and c not in ('Empresa', 'Fecha', 'P_E')]\n",
    "    if not feats:\n",
    "        print(f\"{metodo}: no hay features para lag; se omite.\")\n",
    "        continue\n",
    "\n",
    "    # Preparamos el df base con IDs + target + características\n",
    "    df_base = df_imputed[['Empresa', 'Fecha', 'P_E'] + feats].copy()\n",
    "\n",
    "    # Generamos los lags\n",
    "    df_lagged = create_lags(df_base, 'Empresa', 'Fecha', feats, lag=1)\n",
    "\n",
    "    # Eliminamos filas donde cualquier lag sea NaN\n",
    "    df_lagged.dropna(subset=[f\"{f}_lag1\" for f in feats], inplace=True)\n",
    "\n",
    "    dfs_shifted[metodo] = df_lagged\n",
    "    print(f\"{metodo}: {df_lagged.shape[0]} filas, {df_lagged.shape[1]} columnas\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd16b829",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time # Para medir tiempo\n",
    "import joblib\n",
    "\n",
    "# --- Modelos ---\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# --- Métricas y Preprocesamiento ---\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, make_scorer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import BaseCrossValidator, train_test_split, GridSearchCV, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from pipeline_utils import DropColumns, GroupTimeSeriesSplit\n",
    "# --- IMPORTACIONES PARA TRANSFORMACIONES (Si las usaras en el futuro) ---\n",
    "# from sklearn.preprocessing import PowerTransformer, QuantileTransformer\n",
    "# from scipy.stats import boxcox\n",
    "# from scipy.special import inv_boxcox\n",
    "# ------------------------------------\n",
    "\n",
    "# Ignorar warnings comunes (opcional)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# =====================================\n",
    "# FUNCIONES AUXILIARES (Con mejoras de robustez)\n",
    "# =====================================\n",
    "def rmse(y_true, y_pred):\n",
    "    \"\"\"Calcula RMSE manejando NaNs/Infs.\"\"\"\n",
    "    y_true = np.asarray(y_true); y_pred = np.asarray(y_pred)\n",
    "    mask = np.isfinite(y_true) & np.isfinite(y_pred)\n",
    "    if not np.all(mask): y_true, y_pred = y_true[mask], y_pred[mask]\n",
    "    if len(y_true) == 0: return np.nan\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "rmse_scorer = make_scorer(rmse, greater_is_better=False)\n",
    "\n",
    "\n",
    "def get_n_splits(self, X=None, y=None, groups=None):\n",
    "         if groups is None: return self.n_splits\n",
    "         groups = np.asarray(groups); n_groups = len(np.unique(groups))\n",
    "         return min(self.n_splits, n_groups) if n_groups >= 2 else 0\n",
    "\n",
    "# Función prepare_data ya no se usa\n",
    "\n",
    "def train_test_split_by_empresa(df_model, n_empresas_prueba):\n",
    "    \"\"\"Divide los datos por empresa, asegurando un mínimo para entrenar.\"\"\"\n",
    "    if 'Empresa' not in df_model.columns: raise KeyError(\"'Empresa' necesaria\")\n",
    "    empresas_unicas = df_model['Empresa'].unique()\n",
    "    n_total_empresas = len(empresas_unicas)\n",
    "\n",
    "    # Asegurar que n_empresas_prueba sea un entero válido\n",
    "    if not isinstance(n_empresas_prueba, int) or n_empresas_prueba < 1:\n",
    "         print(f\"Advertencia train_test_split: n_empresas_prueba ({n_empresas_prueba}) debe ser un entero >= 1. Usando 1.\")\n",
    "         n_empresas_prueba = 1\n",
    "\n",
    "    # Asegurar que queden al menos 2 empresas para entrenar\n",
    "    if n_empresas_prueba >= n_total_empresas - 1:\n",
    "        print(f\"Advertencia train_test_split: n_empresas_prueba ({n_empresas_prueba}) es demasiado alto para {n_total_empresas} empresas. Se necesitan al menos 2 para entrenar. Reduciendo n_empresas_prueba.\")\n",
    "        n_empresas_prueba = max(1, n_total_empresas - 2) # Dejar al menos 2 en train\n",
    "        if n_empresas_prueba == 0: # Si solo había 2 empresas en total\n",
    "             print(\"ERROR train_test_split: No hay suficientes empresas para separar train y test.\")\n",
    "             return None, None, None, None\n",
    "\n",
    "    empresas_entrenamiento, empresas_prueba = train_test_split(empresas_unicas, test_size=n_empresas_prueba, random_state=42)\n",
    "    mask_entrenamiento = df_model['Empresa'].isin(empresas_entrenamiento); mask_prueba = df_model['Empresa'].isin(empresas_prueba)\n",
    "    if 'P_E' not in df_model.columns: raise KeyError(\"'P_E' necesaria\")\n",
    "    X = df_model.drop(columns=['P_E']); y = df_model['P_E']\n",
    "    X_train, y_train = X[mask_entrenamiento].copy(), y[mask_entrenamiento].copy()\n",
    "    X_test, y_test = X[mask_prueba].copy(), y[mask_prueba].copy()\n",
    "    if X_train.empty or X_test.empty: return None, None, None, None\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def evaluate_model(y_test, y_pred):\n",
    "    \"\"\" Retorna métricas manejando NaNs/Infs.\"\"\"\n",
    "    y_test, y_pred = np.asarray(y_test), np.asarray(y_pred)\n",
    "    mask = np.isfinite(y_test) & np.isfinite(y_pred)\n",
    "    if not np.all(mask): y_test, y_pred = y_test[mask], y_pred[mask]\n",
    "    if len(y_test) == 0: return {'RMSE': np.nan, 'MAE': np.nan, 'R2': np.nan}\n",
    "    mae = mean_absolute_error(y_test, y_pred); rmse_val = rmse(y_test, y_pred)\n",
    "    r2 = np.nan\n",
    "    if np.var(y_test) > 1e-9:\n",
    "         try: r2 = r2_score(y_test, y_pred)\n",
    "         except ValueError: pass\n",
    "    elif mean_squared_error(y_test, y_pred) < 1e-9: r2 = 1.0\n",
    "    else: r2 = 0.0\n",
    "    return {'RMSE': rmse_val, 'MAE': mae, 'R2': r2}\n",
    "\n",
    "def sort_by_timestep(X, y):\n",
    "    \"\"\"Ordena X e y según 'time_step'.\"\"\"\n",
    "    if 'time_step' not in X.columns: return X, y\n",
    "    if not isinstance(y, pd.Series) or not y.index.equals(X.index):\n",
    "        if len(y) == len(X): y = pd.Series(y, index=X.index, name=getattr(y, 'name', 'target'))\n",
    "        else: print(\"Error sort_by_timestep: y no alineable.\"); return X, y\n",
    "    X_sorted = X.copy().sort_values('time_step'); y_sorted = y.loc[X_sorted.index]\n",
    "    return X_sorted, y_sorted\n",
    "\n",
    "def plot_feature_importances(model, feature_names, title=\"Importancia de Características\"):\n",
    "    \"\"\"Grafica importancia de features.\"\"\"\n",
    "    try:\n",
    "        estimator = model.steps[-1][1] if hasattr(model, 'steps') else model\n",
    "        if not hasattr(estimator, 'feature_importances_'): return None\n",
    "        importances = estimator.feature_importances_\n",
    "        names = getattr(estimator, 'feature_names_in_', feature_names)\n",
    "        if len(names) != len(importances): names = feature_names\n",
    "        if len(names) != len(importances): return None\n",
    "        fi_df = pd.DataFrame({'feature': names, 'importance': importances}).sort_values('importance', ascending=False)\n",
    "        top_n = 30; fi_df = fi_df.head(top_n)\n",
    "        plt.figure(figsize=(10, max(5, len(fi_df) * 0.3))); plt.barh(fi_df['feature'], fi_df['importance'])\n",
    "        plt.xlabel('Importancia'); plt.ylabel('Característica'); plt.title(title); plt.gca().invert_yaxis(); plt.tight_layout(); plt.show()\n",
    "        return fi_df\n",
    "    except Exception as e: print(f\"Error graficando importancia: {e}\"); return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fddc94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# BLOQUE DE EXPERIMENTOS (TODOS LOS MODELOS, LAG 1, DROPNA, LOG TRANSFORM)\n",
    "# =====================================\n",
    "\n",
    "results = []\n",
    "start_time_total = time.time()\n",
    "use_log_transform = True\n",
    "test_size_ratio = 0.20  # porcentaje para test set\n",
    "\n",
    "for metodo, df_lag1 in dfs_shifted.items():\n",
    "    start_time_config = time.time()\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"Procesando Método: {metodo} {'(CON Log Transform)' if use_log_transform else '(SIN Log Transform)'} (Lag 1, dropna)\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "    # Preparar df_model y time_step\n",
    "    df_model = df_lag1.copy()\n",
    "    if 'time_step' not in df_model.columns:\n",
    "        df_model.sort_values(['Empresa','Fecha'], inplace=True)\n",
    "        df_model['time_step'] = df_model.groupby('Empresa').cumcount()\n",
    "\n",
    "    # Detectar lags\n",
    "    all_predictors_lag1 = sorted([c for c in df_model.columns if c.endswith('_lag1')])\n",
    "    print(f\"  ⇨ Número de predictores (_lag1): {len(all_predictors_lag1)}\")\n",
    "    print(f\"  ⇨ Lista de predictores: {all_predictors_lag1}\")\n",
    "\n",
    "    # Split por empresa\n",
    "    n_tot = df_model['Empresa'].nunique()\n",
    "    n_test = max(1, int(round(n_tot * test_size_ratio)))\n",
    "    if n_test >= n_tot - 1:\n",
    "        n_test = max(1, n_tot - 2)\n",
    "    print(f\"    Total empresas: {n_tot}, prueba: {n_test}\")\n",
    "\n",
    "    X_train, y_train, X_test, y_test = train_test_split_by_empresa(df_model, n_test)\n",
    "    if X_train is None:\n",
    "        print(f\"Split fallido para {metodo}.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"  ⇨ X_train.shape: {X_train.shape}\")\n",
    "    print(f\"  ⇨ X_train.columns: {X_train.columns.tolist()}\")\n",
    "\n",
    "    # Log-transform si aplica\n",
    "    if use_log_transform:\n",
    "        y_num = pd.to_numeric(y_train, errors='coerce').fillna(0)\n",
    "        y_train_t = np.log1p(y_num)\n",
    "    else:\n",
    "        y_train_t = y_train\n",
    "\n",
    "    # Ordenar si validación temporal\n",
    "    X_train_s, y_train_s = sort_by_timestep(X_train, y_train_t)\n",
    "    X_test_s,  y_test_o = sort_by_timestep(X_test,  y_test)\n",
    "\n",
    "    # Definir grids\n",
    "    param_grid_rf   = {'rf__max_depth':[10,None],'rf__min_samples_split':[5,10],'rf__n_estimators':[100]}\n",
    "    param_grid_xgb  = {'xgb__n_estimators':[100,200],'xgb__max_depth':[3,5],'xgb__learning_rate':[0.1,0.05]}\n",
    "    param_grid_lgbm = {'lgbm__n_estimators':[100,200],'lgbm__learning_rate':[0.1,0.05],'lgbm__max_depth':[3,5],'lgbm__num_leaves':[15,31]}\n",
    "    param_grid_cb   = {'cb__iterations':[100,200],'cb__learning_rate':[0.1,0.05],'cb__depth':[3,5],'cb__l2_leaf_reg':[1,3]}\n",
    "\n",
    "    scenarios = {\n",
    "        'RF_Simple':  {'model':RandomForestRegressor(42,-1),'hp':False,'vt':False},\n",
    "        'RF_HP':      {'model':RandomForestRegressor(42,-1),'hp':True,'vt':False,'grid':param_grid_rf,'prefix':'rf'},\n",
    "        'XGB_Simple': {'model':XGBRegressor(42,-1),'hp':False,'vt':False},\n",
    "        'XGB_HP':     {'model':XGBRegressor(42,-1),'hp':True,'vt':False,'grid':param_grid_xgb,'prefix':'xgb'},\n",
    "        'LGBM_Simple':{'model':LGBMRegressor(42,-1),'hp':False,'vt':False},\n",
    "        'LGBM_HP':    {'model':LGBMRegressor(42,-1),'hp':True,'vt':False,'grid':param_grid_lgbm,'prefix':'lgbm'},\n",
    "        'CB_Simple':  {'model':CatBoostRegressor(42,verbose=0,allow_writing_files=False),'hp':False,'vt':False},\n",
    "        'CB_HP':      {'model':CatBoostRegressor(42,verbose=0,allow_writing_files=False),'hp':True,'vt':False,'grid':param_grid_cb,'prefix':'cb'},\n",
    "    }\n",
    "\n",
    "    for esc, cfg in scenarios.items():\n",
    "        t0 = time.time()\n",
    "        print(f\"\\n  --- Escenario: {esc} ---\")\n",
    "        base = cfg['model']\n",
    "        hp   = cfg['hp']\n",
    "        vt   = cfg['vt']\n",
    "        Xtr = X_train_s if vt else X_train\n",
    "        ytr = y_train_s if vt else y_train_t\n",
    "        Xte = X_test_s  if vt else X_test\n",
    "        yto = y_test_o  if vt else y_test\n",
    "\n",
    "        drop_cols = [c for c in ['Empresa','Fecha','time_step'] if c in Xtr.columns]\n",
    "        steps = [('drop',DropColumns(drop_cols))]\n",
    "\n",
    "        if hp:\n",
    "            steps.append((cfg['prefix'], base))\n",
    "            pipe = Pipeline(steps)\n",
    "            ng = Xtr['Empresa'].nunique()\n",
    "            if vt and ng>=2:\n",
    "                cv = GroupTimeSeriesSplit(n_splits=min(4,ng))\n",
    "                fitp={'groups':Xtr['Empresa']}\n",
    "            else:\n",
    "                cv = KFold(3,shuffle=True,random_state=42)\n",
    "                fitp={}\n",
    "            gs = GridSearchCV(pipe, cfg['grid'], scoring=rmse_scorer,\n",
    "                              cv=cv, n_jobs=1, refit=True, error_score='raise')\n",
    "            gs.fit(Xtr, ytr, **fitp)\n",
    "            est = gs.best_estimator_.steps[-1][1]\n",
    "            print(f\"    >> n_features_in_={est.n_features_in_}\")\n",
    "            modelo = gs.best_estimator_\n",
    "        else:\n",
    "            steps.append(('mdl',base))\n",
    "            pipe = Pipeline(steps)\n",
    "            pipe.fit(Xtr,ytr)\n",
    "            est = pipe.steps[-1][1]\n",
    "            print(f\"    >> n_features_in_={est.n_features_in_}\")\n",
    "            modelo = pipe\n",
    "\n",
    "        # predicción\n",
    "        pr = modelo.predict(Xte)\n",
    "        if use_log_transform:\n",
    "            with np.errstate(over='ignore'): pr = np.expm1(pr)\n",
    "            pr[~np.isfinite(pr)] = np.nan\n",
    "\n",
    "        # evaluación\n",
    "        res = evaluate_model(yto, pr)\n",
    "        res.update({\n",
    "            'Modelo':esc.split('_')[0],\n",
    "            'Validacion_Temp':vt,'Busqueda_HP':hp,\n",
    "            'Config_Key':key,'Metodo':metodo,\n",
    "            'Lag_Config':'lag1','Log_Transform':use_log_transform,\n",
    "            'Best_Params': gs.best_params_ if hp else None,\n",
    "            'pipeline':modelo\n",
    "        })\n",
    "        print(f\"    Métricas {esc}: {res}\")\n",
    "        results.append(res)\n",
    "        print(f\"    Tiempo escenario: {time.time()-t0:.2f}s\")\n",
    "\n",
    "    print(f\"\\nTiempo total {metodo}: {time.time()-start_time_config:.2f}s\")\n",
    "\n",
    "print(f\"\\nTiempo total ejecución: {(time.time()-start_time_total)/60:.2f}min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8875320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# CREAR TABLA DE RESULTADOS\n",
    "# =====================================\n",
    "if results:\n",
    "    # Armamos DataFrame con todos los resultados acumulados\n",
    "    df_results_all_models = pd.DataFrame(results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"--- Resultados Finales (Todos los Modelos) ---\")\n",
    "    print(f\"Total de resultados generados: {len(df_results_all_models)}\\n\")\n",
    "    \n",
    "    # Definimos las columnas que queremos mostrar\n",
    "    cols_display = [\n",
    "        'Config_Key', 'Modelo', 'Validacion_Temp', 'Busqueda_HP',\n",
    "        'Log_Transform', 'Lag_Config', 'RMSE', 'MAE', 'R2', 'Best_Params'\n",
    "    ]\n",
    "    \n",
    "    # Ordenamos por RMSE ascendente y mostramos la tabla\n",
    "    with pd.option_context('display.max_rows', None,\n",
    "                           'display.max_columns', None,\n",
    "                           'display.width', 1000):\n",
    "        print(\n",
    "            df_results_all_models\n",
    "            .sort_values(by='RMSE', ascending=True, na_position='last')\n",
    "            [cols_display]\n",
    "            .to_string(index=False)\n",
    "        )\n",
    "else:\n",
    "    print(\"\\nNo se generaron resultados.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cd9c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTabla de Resultados:\")\n",
    "df_results_all_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0df6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elección entre modelos\n",
    "\n",
    "def select_candidate_models(df, n=3):\n",
    "    \"\"\"\n",
    "    Selecciona los n modelos candidatos basándose únicamente en el RMSE (de menor a mayor).\n",
    "    \"\"\"\n",
    "    return df.sort_values('RMSE').head(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f296f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Candidatos a mejor modelo\n",
    "top_candidates = select_candidate_models(df_results_all_models, n=3)\n",
    "print(\"Top candidatos según RMSE\")\n",
    "top_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef2f21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar el mejor pipeline y guardarlo\n",
    "best_idx = df_results_all_models['RMSE'].idxmin()\n",
    "best_pipeline = df_results_all_models.loc[best_idx, 'pipeline']\n",
    "joblib.dump(best_pipeline, 'best_pipeline_model_.pkl')\n",
    "print(\"Modelo ganador guardado en best_pipeline_model_.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db756191",
   "metadata": {},
   "source": [
    "## Dlocal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29ef091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los datos desde el archivo Excel\n",
    "file_path = 'C:/Users/Andy/OneDrive/Desktop/MCD/Tesis/Datos_fuente_Bloomberg/en valores/serie completa 2014-2024/Dataset/dataset_dlocal_casi sin vacías.xlsx'\n",
    "df_dlocal = pd.read_excel(file_path, sheet_name='dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43fc10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dlocal.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a98e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quitar \"k\" de las columnas \"Non farm payrolls_Exp_mediana\" y \"Non farm payrolls_Exp_promedio\"\n",
    "df_dlocal['Non farm payrolls_Exp_mediana'] = df_dlocal['Non farm payrolls_Exp_mediana'].astype(str).str.replace('k', '')\n",
    "df_dlocal['Non farm payrolls_Exp_promedio'] = df_dlocal['Non farm payrolls_Exp_promedio'].astype(str).str.replace('k', '')\n",
    "\n",
    "# Convertir las columnas a valores numéricos\n",
    "df_dlocal['Non farm payrolls_Exp_mediana'] = pd.to_numeric(df_dlocal['Non farm payrolls_Exp_mediana'], errors='coerce')\n",
    "df_dlocal['Non farm payrolls_Exp_promedio'] = pd.to_numeric(df_dlocal['Non farm payrolls_Exp_promedio'], errors='coerce')\n",
    "\n",
    "df_dlocal['P_B'] = pd.to_numeric(df_dlocal['P_B'], errors='coerce')\n",
    "#df_dlocal['P_TB'] = pd.to_numeric(df_dlocal['P_TB'], errors='coerce')\n",
    "df_dlocal['P_S'] = pd.to_numeric(df_dlocal['P_S'], errors='coerce')\n",
    "#df_dlocal['P_FCF'] = pd.to_numeric(df_dlocal['P_FCF'], errors='coerce')\n",
    "df_dlocal['P_Share'] = pd.to_numeric(df_dlocal['P_Share'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a507a489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir la columna de fecha a formato datetime\n",
    "df_dlocal['Fecha'] = pd.to_datetime(df_dlocal['Fecha'], format='%Y%m%d')\n",
    "\n",
    "# Extraer características temporales relevantes\n",
    "df_dlocal['mes'] = df_dlocal['Fecha'].dt.month\n",
    "df_dlocal['año'] = df_dlocal['Fecha'].dt.year\n",
    "df_dlocal['periodo'] = df_dlocal['Fecha'].dt.to_period('M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c7eb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir CPI y Fed Funds Rate a decimales\n",
    "df_dlocal['CPI'] = df_dlocal['CPI'] / 100\n",
    "df_dlocal['Fed Funds Rate'] = df_dlocal['Fed Funds Rate'] / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5d58a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular diferencias en puntos básicos\n",
    "df_dlocal['dif_CPI_mediana'] = (df_dlocal['CPI'] - df_dlocal['CPI_Exp_mediana']) * 10000\n",
    "df_dlocal['dif_CPI_promedio'] = (df_dlocal['CPI'] - df_dlocal['CPI_Exp_promedio']) * 10000\n",
    "\n",
    "df_dlocal['dif_FFR_mediana'] = (df_dlocal['Fed Funds Rate'] - df_dlocal['Fed Funds Rate_Exp_mediana']) * 10000\n",
    "df_dlocal['dif_FFR_promedio'] = (df_dlocal['Fed Funds Rate'] - df_dlocal['Fed Funds Rate_Exp_promedio']) * 10000\n",
    "\n",
    "# Calcular diferencia\n",
    "df_dlocal['dif_NFP_mediana'] = (df_dlocal['Non farm payrolls'] - df_dlocal['Non farm payrolls_Exp_mediana'])\n",
    "df_dlocal['dif_NFP_promedio'] = (df_dlocal['Non farm payrolls'] - df_dlocal['Non farm payrolls_Exp_promedio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa84a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supongamos que loaded es tu pipeline cargado\n",
    "from joblib import load\n",
    "loaded = load(\"best_pipeline_model_.pkl\")\n",
    "\n",
    "# Extrae el modelo final (el regresor) — si es GridSearchCV, ajusta el acceso\n",
    "if hasattr(loaded, \"best_estimator_\"):\n",
    "    reg = loaded.best_estimator_.steps[-1][1]\n",
    "elif hasattr(loaded, \"steps\"):\n",
    "    reg = loaded.steps[-1][1]\n",
    "else:\n",
    "    reg = loaded  # por si guardaste solo el estimador\n",
    "\n",
    "print(\"Modelo final:\", reg)\n",
    "print(\"Número de features que vio en fit:\", reg.n_features_in_)\n",
    "print(\"Nombres de features (si los tiene):\", getattr(reg, \"feature_names_in_\", None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9b6a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time # Para medir tiempo\n",
    "import joblib # Para cargar el modelo\n",
    "\n",
    "# --- Modelos (Solo necesarios si el pipeline no los incluye serializados) ---\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from xgboost import XGBRegressor\n",
    "# from lightgbm import LGBMRegressor\n",
    "# from catboost import CatBoostRegressor\n",
    "\n",
    "# --- Métricas y Preprocesamiento ---\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, make_scorer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import BaseCrossValidator, train_test_split, GridSearchCV, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "# --- IMPORTACIONES PARA TRANSFORMACIONES (Si las usaras en el futuro) ---\n",
    "# from sklearn.preprocessing import PowerTransformer, QuantileTransformer\n",
    "# from scipy.stats import boxcox\n",
    "# from scipy.special import inv_boxcox\n",
    "# ------------------------------------\n",
    "\n",
    "# Ignorar warnings comunes (opcional)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# =====================================\n",
    "# FUNCIONES AUXILIARES (Requeridas por el código)\n",
    "# =====================================\n",
    "def rmse(y_true, y_pred):\n",
    "    \"\"\"Calcula RMSE manejando NaNs/Infs.\"\"\"\n",
    "    y_true = np.asarray(y_true); y_pred = np.asarray(y_pred)\n",
    "    mask = np.isfinite(y_true) & np.isfinite(y_pred)\n",
    "    if not np.all(mask): y_true, y_pred = y_true[mask], y_pred[mask]\n",
    "    if len(y_true) == 0: return np.nan\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def evaluate_model(y_test, y_pred):\n",
    "    \"\"\" Retorna métricas manejando NaNs/Infs.\"\"\"\n",
    "    y_test, y_pred = np.asarray(y_test), np.asarray(y_pred)\n",
    "    mask = np.isfinite(y_test) & np.isfinite(y_pred)\n",
    "    if not np.all(mask): y_test, y_pred = y_test[mask], y_pred[mask]\n",
    "    if len(y_test) == 0: return {'RMSE': np.nan, 'MAE': np.nan, 'R2': np.nan}\n",
    "    mae = mean_absolute_error(y_test, y_pred); rmse_val = rmse(y_test, y_pred)\n",
    "    r2 = np.nan\n",
    "    if np.var(y_test) > 1e-9:\n",
    "         try: r2 = r2_score(y_test, y_pred)\n",
    "         except ValueError: pass\n",
    "    elif mean_squared_error(y_test, y_pred) < 1e-9: r2 = 1.0\n",
    "    else: r2 = 0.0\n",
    "    return {'RMSE': rmse_val, 'MAE': mae, 'R2': r2}\n",
    "\n",
    "class DropColumns(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Transformer para eliminar columnas especificadas en un Pipeline.\"\"\"\n",
    "    def __init__(self, columns=None): self.columns = columns\n",
    "    def fit(self, X, y=None): return self\n",
    "    def transform(self, X):\n",
    "        if not isinstance(X, pd.DataFrame): return X\n",
    "        return X.drop(columns=self.columns, errors='ignore')\n",
    "\n",
    "# =====================================\n",
    "# BLOQUE DE PREDICCIÓN PARA DLOCAL (CORREGIDO - SIN BFILL PRE-LAG)\n",
    "# =====================================\n",
    "\n",
    "# --- CONFIGURACIÓN ---\n",
    "# ASUME df_dlocal: DataFrame cargado y con preprocesamiento inicial\n",
    "# ASUME listas_columnas: Diccionario {metodo: [lista_cols_base]}\n",
    "# ASUME que el mejor modelo fue entrenado con transformación logarítmica y dropna\n",
    "\n",
    "use_log_transform = True # ¡¡ASEGÚRATE de que esto coincida con cómo entrenaste el modelo cargado!!\n",
    "metodo_modelo_cargado = 'lineal_promedio' # <-- ¡¡AJUSTA ESTO AL MÉTODO DE TU MEJOR MODELO!!\n",
    "nombre_archivo_modelo = \"best_pipeline_model_.pkl\" # <-- ¡¡PONE EL NOMBRE CORRECTO DE TU ARCHIVO GUARDADO!!\n",
    "\n",
    "# --- Verificar Inputs Necesarios ---\n",
    "if 'df_dlocal' not in locals() or not isinstance(df_dlocal, pd.DataFrame):\n",
    "     raise NameError(\"El DataFrame 'df_dlocal' no está definido.\")\n",
    "if 'listas_columnas' not in locals() or not isinstance(listas_columnas, dict):\n",
    "     raise NameError(\"El diccionario 'listas_columnas' no está definido.\")\n",
    "if metodo_modelo_cargado not in listas_columnas:\n",
    "    raise ValueError(f\"El método '{metodo_modelo_cargado}' no se encuentra en listas_columnas.\")\n",
    "# ---------------------------------\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"Preparando datos de DLocal para predicción (Método: {metodo_modelo_cargado})\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# --- 1. Preparar df_dlocal con Lags (usando dropna) ---\n",
    "df_dlocal_preparado = df_dlocal.copy()\n",
    "\n",
    "# --- 1.1 Forzar Conversión Numérica ---\n",
    "columnas_base_modelo_pe = listas_columnas[metodo_modelo_cargado]\n",
    "columnas_a_convertir = [c for c in columnas_base_modelo_pe if c not in ['Empresa', 'Fecha']]\n",
    "columnas_a_convertir_existentes = [c for c in columnas_a_convertir if c in df_dlocal_preparado.columns]\n",
    "print(f\"Forzando {len(columnas_a_convertir_existentes)} columnas a numérico en DLocal...\")\n",
    "for col in columnas_a_convertir_existentes:\n",
    "    if not pd.api.types.is_numeric_dtype(df_dlocal_preparado[col]):\n",
    "        df_dlocal_preparado[col] = pd.to_numeric(df_dlocal_preparado[col], errors='coerce')\n",
    "\n",
    "# --- 1.2 Imputar NaNs PRE-LAG usando FFILL + FILLNA(0) ---\n",
    "print(\"Imputando NaNs pre-lag con ffill + fillna(0)...\")\n",
    "cols_imputar_prelag = [c for c in df_dlocal_preparado.columns if c not in ['Empresa', 'Fecha']]\n",
    "cols_imputar_prelag_existentes = [c for c in cols_imputar_prelag if c in df_dlocal_preparado.columns]\n",
    "df_dlocal_preparado.sort_values(['Empresa', 'Fecha'], inplace=True)\n",
    "# Aplicar ffill por grupo (asumiendo que df_dlocal solo tiene una 'Empresa')\n",
    "if df_dlocal_preparado['Empresa'].nunique() > 1:\n",
    "     df_dlocal_preparado[cols_imputar_prelag_existentes] = df_dlocal_preparado.groupby('Empresa')[cols_imputar_prelag_existentes].ffill()\n",
    "else:\n",
    "     df_dlocal_preparado[cols_imputar_prelag_existentes] = df_dlocal_preparado[cols_imputar_prelag_existentes].ffill()\n",
    "# Rellenar NaNs restantes (iniciales) con 0\n",
    "nans_post_ffill = df_dlocal_preparado[cols_imputar_prelag_existentes].isnull().sum().sum()\n",
    "if nans_post_ffill > 0:\n",
    "    print(f\"    {nans_post_ffill} NaNs restantes después de ffill. Rellenando con 0.\")\n",
    "    df_dlocal_preparado.fillna(0, inplace=True) # Relleno global final\n",
    "else:\n",
    "    print(\"    No quedan NaNs después de ffill.\")\n",
    "# --- NO SE USA BFILL ---\n",
    "\n",
    "# --- 1.3 Seleccionar Columnas y Generar Lag 1 ---\n",
    "columnas_base_modelo = [c for c in listas_columnas[metodo_modelo_cargado] if c != 'P_E']\n",
    "cols_necesarias_dlocal = ['Empresa', 'Fecha', 'P_E'] + [c for c in columnas_base_modelo if c in df_dlocal_preparado.columns]\n",
    "df_dlocal_filtrado = df_dlocal_preparado[cols_necesarias_dlocal].copy()\n",
    "\n",
    "predictors_dlocal = []\n",
    "print(f\"  Generando lag 1 para {len(columnas_base_modelo)} features base...\")\n",
    "for col_base in columnas_base_modelo:\n",
    "    if col_base in df_dlocal_filtrado.columns:\n",
    "        lag_col_name = f\"{col_base}_lag1\"\n",
    "        # Usar groupby aunque sea una empresa para consistencia\n",
    "        df_dlocal_filtrado[lag_col_name] = df_dlocal_filtrado.groupby('Empresa')[col_base].shift(1)\n",
    "        predictors_dlocal.append(lag_col_name)\n",
    "\n",
    "# --- 1.4 Aplicar dropna ---\n",
    "print(\"  Aplicando dropna() a las columnas de lag...\")\n",
    "rows_before = len(df_dlocal_filtrado)\n",
    "df_dlocal_lagged = df_dlocal_filtrado.dropna(subset=predictors_dlocal).copy()\n",
    "rows_after = len(df_dlocal_lagged)\n",
    "print(f\"  Se eliminaron {rows_before - rows_after} filas.\")\n",
    "print(f\"  Shape final de datos DLocal con lags: {df_dlocal_lagged.shape}\")\n",
    "\n",
    "if df_dlocal_lagged.empty:\n",
    "    raise ValueError(\"El DataFrame de DLocal quedó vacío después de calcular lags y aplicar dropna.\")\n",
    "\n",
    "# Añadir time_step si falta (para consistencia con entrenamiento, aunque no se use en predict)\n",
    "if 'time_step' not in df_dlocal_lagged.columns:\n",
    "    df_dlocal_lagged.sort_values(['Empresa', 'Fecha'], inplace=True)\n",
    "    df_dlocal_lagged['time_step'] = df_dlocal_lagged.groupby('Empresa').cumcount()\n",
    "\n",
    "# --- 2. Cargar el Modelo Entrenado ---\n",
    "print(f\"\\nCargando modelo entrenado desde: {nombre_archivo_modelo}\")\n",
    "try:\n",
    "    loaded_best_pipeline = joblib.load(nombre_archivo_modelo)\n",
    "    print(\"Modelo cargado exitosamente.\")\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(f\"Error: No se encontró el archivo del modelo '{nombre_archivo_modelo}'.\")\n",
    "except Exception as e_load:\n",
    "    raise RuntimeError(f\"Error al cargar el modelo: {e_load}\")\n",
    "\n",
    "# --- 3. Obtener Features Esperadas por el Modelo Cargado ---\n",
    "print(\"\\nVerificando modelo cargado y features esperadas...\")\n",
    "expected_features = None\n",
    "try:\n",
    "    final_model = None\n",
    "    if hasattr(loaded_best_pipeline, 'steps'):\n",
    "        final_model = loaded_best_pipeline.steps[-1][1]\n",
    "    elif hasattr(loaded_best_pipeline, 'feature_names_in_'):\n",
    "        final_model = loaded_best_pipeline\n",
    "\n",
    "    if final_model and hasattr(final_model, 'feature_names_in_'):\n",
    "        expected_features = list(final_model.feature_names_in_)\n",
    "        print(f\"Modelo cargado espera {len(expected_features)} features.\")\n",
    "    elif final_model and hasattr(final_model, 'n_features_in_'):\n",
    "         num_expected = final_model.n_features_in_\n",
    "         print(f\"Modelo cargado espera {num_expected} features (nombres no disponibles).\")\n",
    "         expected_features = sorted([p for p in predictors_dlocal if p in df_dlocal_lagged.columns])\n",
    "         if len(expected_features) != num_expected: print(f\"¡ADVERTENCIA! Discrepancia features: Modelo={num_expected}, Generadas={len(expected_features)}.\")\n",
    "         print(f\"Usando {len(expected_features)} features generadas.\")\n",
    "    else:\n",
    "         print(\"\\nAdvertencia: No se pudo determinar las features esperadas. Usando todas las _lag1 generadas.\")\n",
    "         expected_features = sorted([p for p in predictors_dlocal if p in df_dlocal_lagged.columns])\n",
    "         if not expected_features: raise ValueError(\"No se pudieron determinar las features.\")\n",
    "         print(f\"Intentando usar {len(expected_features)} features generadas.\")\n",
    "\n",
    "    missing_features_for_pred = [f for f in expected_features if f not in df_dlocal_lagged.columns]\n",
    "    if missing_features_for_pred:\n",
    "        raise ValueError(f\"Faltan features requeridas en DLocal: {missing_features_for_pred}\")\n",
    "    print(\"Todas las features esperadas están presentes.\")\n",
    "\n",
    "except Exception as e_model_check:\n",
    "    raise ValueError(f\"Error al verificar el modelo cargado: {e_model_check}\")\n",
    "\n",
    "\n",
    "# --- 4. Predicción One-Step Ahead Retrospectiva y Forecast ---\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Predicción One-Step Ahead para DLocal\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "n_rows = len(df_dlocal_lagged)\n",
    "fechas_pred = []\n",
    "predicciones = []\n",
    "valores_reales = []\n",
    "\n",
    "if n_rows < 1:\n",
    "    print(\"No hay suficientes filas en df_dlocal_lagged para hacer predicciones.\")\n",
    "else:\n",
    "    # Bucle retrospectivo\n",
    "    print(f\"Realizando {n_rows-1} predicciones retrospectivas...\")\n",
    "    for i in range(1, n_rows):\n",
    "        features_input_row = df_dlocal_lagged.iloc[[i-1]]\n",
    "        X_pred = features_input_row[expected_features] # Seleccionar SOLO las features esperadas\n",
    "\n",
    "        try:\n",
    "            pred_raw = loaded_best_pipeline.predict(X_pred)[0]\n",
    "            if use_log_transform:\n",
    "                with np.errstate(over='ignore', invalid='ignore'): pred_orig = np.expm1(pred_raw)\n",
    "                if not np.isfinite(pred_orig): pred_orig = np.nan\n",
    "            else:\n",
    "                pred_orig = pred_raw\n",
    "            predicciones.append(pred_orig)\n",
    "            fechas_pred.append(df_dlocal_lagged.iloc[i]['Fecha'])\n",
    "            valores_reales.append(df_dlocal_lagged.iloc[i]['P_E'])\n",
    "        except Exception as e_pred:\n",
    "            print(f\"Error prediciendo para fila {i} (Fecha: {df_dlocal_lagged.iloc[i]['Fecha']}): {e_pred}\")\n",
    "            predicciones.append(np.nan)\n",
    "            fechas_pred.append(df_dlocal_lagged.iloc[i]['Fecha'])\n",
    "            valores_reales.append(df_dlocal_lagged.iloc[i]['P_E'])\n",
    "\n",
    "    # Forecast para el siguiente período\n",
    "    print(\"Generando forecast para el próximo período...\")\n",
    "    features_ultimo = df_dlocal_lagged.iloc[[-1]]\n",
    "    X_pred_ultimo = features_ultimo[expected_features] # Seleccionar SOLO las features esperadas\n",
    "    pronostico_next = np.nan # Default\n",
    "\n",
    "    try:\n",
    "        pronostico_next_raw = loaded_best_pipeline.predict(X_pred_ultimo)[0]\n",
    "        if use_log_transform:\n",
    "            with np.errstate(over='ignore', invalid='ignore'): pronostico_next = np.expm1(pronostico_next_raw)\n",
    "            if not np.isfinite(pronostico_next): pronostico_next = np.nan\n",
    "        else:\n",
    "            pronostico_next = pronostico_next_raw\n",
    "    except Exception as e_forecast:\n",
    "        print(f\"Error durante el forecast: {e_forecast}\")\n",
    "\n",
    "    # Estimar la fecha del próximo período\n",
    "    try:\n",
    "        proxima_fecha = df_dlocal_lagged['Fecha'].iloc[-1] + pd.DateOffset(months=1)\n",
    "    except:\n",
    "        proxima_fecha = \"Periodo Siguiente\"\n",
    "\n",
    "    # --- Crear DataFrames de Resultados ---\n",
    "    df_backtest = pd.DataFrame({'Fecha': fechas_pred, 'P_E_Predicho': predicciones, 'P_E_Real': valores_reales})\n",
    "    df_forecast = pd.DataFrame({'Fecha': [proxima_fecha], 'P_E_Predicho': [pronostico_next]})\n",
    "\n",
    "    # --- Mostrar Resultados ---\n",
    "    print(\"\\n--- Resultados del Backtest para DLocal ---\")\n",
    "    with pd.option_context('display.max_rows', None):\n",
    "         print(df_backtest.to_string(float_format=\"%.4f\"))\n",
    "    print(f\"\\n--- Pronóstico para {proxima_fecha} ---\")\n",
    "    print(df_forecast.to_string(float_format=\"%.4f\"))\n",
    "\n",
    "    # --- Calcular Métricas del Backtest ---\n",
    "    print(\"\\n--- Métricas del Backtest (DLocal) ---\")\n",
    "    metricas_dlocal = evaluate_model(df_backtest['P_E_Real'], df_backtest['P_E_Predicho'])\n",
    "    print(f\"RMSE: {metricas_dlocal['RMSE']:.4f}\")\n",
    "    print(f\"MAE:  {metricas_dlocal['MAE']:.4f}\")\n",
    "    print(f\"R2:   {metricas_dlocal['R2']:.4f}\")\n",
    "\n",
    "    # --- Graficar Resultados ---\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(df_backtest['Fecha'], df_backtest['P_E_Real'], marker='.', linestyle='-', label='P/E Real DLocal')\n",
    "    plt.plot(df_backtest['Fecha'], df_backtest['P_E_Predicho'], marker='x', linestyle='--', label='P/E Predicho (Modelo)')\n",
    "    if isinstance(proxima_fecha, pd.Timestamp) and pd.notna(pronostico_next): # Graficar solo si es válido\n",
    "         plt.scatter([proxima_fecha], [pronostico_next], color='red', label=f'Forecast {proxima_fecha.strftime(\"%Y-%m\")}', zorder=5, s=100)\n",
    "    plt.title(f'Backtest y Forecast P/E para DLocal (Modelo: {metodo_modelo_cargado})')\n",
    "    plt.xlabel('Fecha'); plt.ylabel('P/E Ratio'); plt.legend(); plt.grid(True); plt.tight_layout(); plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
